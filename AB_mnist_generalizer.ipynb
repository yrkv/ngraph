{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "import importlib\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from NeuralGraph import NeuralGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "SIZE = 7\n",
    "\n",
    "# Digits 0-7 are used for training\n",
    "train = {i:[] for i in range(8)}\n",
    "# Digits 8 and 9 are used for testing\n",
    "test = {i:[] for i in range(8, 10)}\n",
    "\n",
    "for img, label in zip(x_train, y_train):\n",
    "    if label < 8:\n",
    "        train[label].append(cv2.resize(img, dsize=[SIZE, SIZE]).reshape(SIZE**2))\n",
    "    else:\n",
    "        test[label].append(cv2.resize(img, dsize=[SIZE, SIZE]).reshape(SIZE**2))\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "# Use fashion MNIST for out of distribution testing too\n",
    "fashion_test = {i:[] for i in range(10)}\n",
    "\n",
    "for img, label in zip(x_train, y_train):\n",
    "    fashion_test[label].append(cv2.resize(img, dsize=[SIZE, SIZE]).reshape(SIZE**2))\n",
    "\n",
    "for label in train.keys():\n",
    "    train[label] = np.array(train[label])\n",
    "\n",
    "for label in test.keys():\n",
    "    test[label] = np.array(test[label])\n",
    "\n",
    "for label in fashion_test.keys():\n",
    "    fashion_test[label] = np.array(fashion_test[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([8, 9])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train.keys())\n",
    "plt.imshow(train[0][0].reshape(SIZE, SIZE))\n",
    "plt.show()\n",
    "\n",
    "print(test.keys())\n",
    "plt.imshow(test[8][0].reshape(SIZE, SIZE))\n",
    "plt.show()\n",
    "\n",
    "print(fashion_test.keys())\n",
    "plt.imshow(fashion_test[0][0].reshape(SIZE, SIZE))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2601\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "TIME = 1\n",
    "DT = .25\n",
    "\n",
    "n_classes = 2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Dense network with 49 nodes\n",
    "n_nodes = SIZE**2+n_classes\n",
    "connections = [(x, y) for x in range(n_nodes) for y in range(n_nodes)]\n",
    "print(len(connections))\n",
    "\n",
    "\n",
    "# GRAPH WAS INITIALLY TRAINED WITH DECAY = .5 AND SLOWLY RAMPED TO DECAY = .2\n",
    "# IF TRAINING FROM SCRATCH SET DECAY = .5 AND IF LOADING SET DECAY = .2\n",
    "graph = NeuralGraph(n_nodes, SIZE**2, n_classes, connections, \n",
    "    leakage=.25, ch_n=8, ch_e=8, ch_k=8, value_init=\"trainable\", init_value_std=.1, \n",
    "    clamp_mode=\"hard\", max_value=100, # BC I trained with an old version on ngraph\n",
    "    aggregation=\"attention\", n_heads=1, use_label=True, device=device, n_models=1, decay=.2)\n",
    "\n",
    "log = []\n",
    "\n",
    "graph.load(\"models/generalize.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(graph.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMON Task\n",
    "##### Repeat input/output pairs in order they're given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS = 2_000\n",
    "EXAMPLES = 10\n",
    "log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb96702424494af990a416973f6bf4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m y_input \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mone_hot(y_train, n_classes)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m---> 12\u001b[0m graph\u001b[39m.\u001b[39mlearn(x_train, y_input, time\u001b[39m=\u001b[39mTIME, dt\u001b[39m=\u001b[39mDT)\n\u001b[0;32m     13\u001b[0m pred \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39mpredict(x_train, time\u001b[39m=\u001b[39mTIME, dt\u001b[39m=\u001b[39mDT)\n\u001b[0;32m     15\u001b[0m accs \u001b[39m=\u001b[39m (pred\u001b[39m.\u001b[39margmax(\u001b[39m2\u001b[39m) \u001b[39m==\u001b[39m y_train)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\Alec\\Documents\\GitHub\\ngraph_lang\\NeuralGraph.py:584\u001b[0m, in \u001b[0;36mNeuralGraph.learn\u001b[1;34m(self, X, Y, time, dt, reset_nodes, **kwargs)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_vals(indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool, nodes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, edges\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(X[:, i], dt\u001b[39m=\u001b[39mdt, time\u001b[39m=\u001b[39mtime, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward(X[:, i], Y[:, i], dt\u001b[39m=\u001b[39mdt, time\u001b[39m=\u001b[39mtime, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Alec\\Documents\\GitHub\\ngraph_lang\\NeuralGraph.py:517\u001b[0m, in \u001b[0;36mNeuralGraph.backward\u001b[1;34m(self, x, y, time, dt, apply_once, nodes, edges, edges_at_end)\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimestep(step_nodes\u001b[39m=\u001b[39mnodes, step_edges\u001b[39m=\u001b[39medges, dt\u001b[39m=\u001b[39mdt, t\u001b[39m=\u001b[39mt)\n\u001b[0;32m    515\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_outputs()\n\u001b[1;32m--> 517\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, x, y, time\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, dt\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, apply_once\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, nodes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, edges\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, edges_at_end\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    518\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[39m    Takes an input x and a label y and puts them in the graph and runs the graph for a certain amount of time with a certain time resolution.\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[39m    :param x: Input for the graph.  Must be shape (batch_size, n_inputs, ch_inp) unless ch_inp == 1 \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[39m    :param edges_at_end: Whether to update edges on the last timestep\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    532\u001b[0m     timesteps \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(time \u001b[39m/\u001b[39m dt)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bar = tqdm(range(STEPS))\n",
    "for _ in bar:\n",
    "    x_train = torch.rand(BATCH_SIZE, EXAMPLES, SIZE**2, device=device)\n",
    "    y_train = torch.randint(n_classes, size=(BATCH_SIZE, EXAMPLES), device=device)\n",
    "\n",
    "    graph.init_vals(nodes=True, edges=True, batch_size=BATCH_SIZE)\n",
    "    graph.detach_vals()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_input = nn.functional.one_hot(y_train, n_classes).float()\n",
    "\n",
    "    graph.learn(x_train, y_input, time=TIME, dt=DT)\n",
    "    pred = graph.predict(x_train, time=TIME, dt=DT)\n",
    "\n",
    "    accs = (pred.argmax(2) == y_train).float()\n",
    "\n",
    "    task_loss = criterion(pred, y_input)\n",
    "    overflow = graph.overflow()\n",
    "\n",
    "    loss = task_loss + overflow\n",
    "    loss.backward()\n",
    "\n",
    "    if overflow < .1 and graph.decay >= .25:\n",
    "        graph.decay -= .05\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(graph.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    entry = {'loss': task_loss.item(), 'acc': accs.mean().item(), \"overflow\": overflow.item(), \"decay\":graph.decay}\n",
    "    log.append(entry)\n",
    "    bar.set_postfix({\"loss\":np.mean([e[\"loss\"] for e in log[-10:]]), \"acc\":np.mean([e[\"acc\"] for e in log[-10:]]), \"overflow\":entry[\"overflow\"], \"decay\":entry[\"decay\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.save(\"models/simon.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memorization Task\n",
    "##### Given input/output pairs, give output from corresponding input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES = 10\n",
    "STEPS = 5_000\n",
    "# Reset optimizer\n",
    "optimizer = torch.optim.Adam(graph.parameters(), lr=1e-3)\n",
    "\n",
    "log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = tqdm(range(STEPS))\n",
    "for _ in bar:\n",
    "    x_train = torch.rand(BATCH_SIZE, EXAMPLES, SIZE**2, device=device)\n",
    "    y_train = torch.randint(n_classes, size=(BATCH_SIZE, EXAMPLES), device=device)\n",
    "\n",
    "    graph.init_vals(nodes=True, edges=True, batch_size=BATCH_SIZE)\n",
    "    graph.detach_vals()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_input = nn.functional.one_hot(y_train, n_classes).float()\n",
    "\n",
    "    graph.learn(x_train, y_input, time=TIME, dt=DT)\n",
    "    \n",
    "    shuffled = np.arange(EXAMPLES)\n",
    "    np.random.shuffle(shuffled)\n",
    "    shuffled = torch.Tensor(shuffled).long().to(device)\n",
    "    \n",
    "    pred = graph.predict(x_train[:, shuffled], time=TIME, dt=DT)\n",
    "\n",
    "    accs = (pred.argmax(2) == y_train[:, shuffled]).float()\n",
    "\n",
    "    task_loss = criterion(pred, y_input[:, shuffled])\n",
    "    overflow = graph.overflow()\n",
    "\n",
    "    loss = task_loss + overflow\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(graph.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    entry = {'loss': task_loss.item(), 'acc': accs.mean().item(), \"overflow\": overflow.item(), \"decay\":graph.decay}\n",
    "    log.append(entry)\n",
    "    bar.set_postfix({\"loss\":np.mean([e[\"loss\"] for e in log[-10:]]), \"acc\":np.mean([e[\"acc\"] for e in log[-10:]]), \"overflow\":entry[\"overflow\"], \"decay\":entry[\"decay\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.save(\"models/memorize.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization Task\n",
    "##### Given input/output pairs from a specific task, given output from unseen input data from that task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EXAMPLES = 10\n",
    "TEST_EXAMPLES = 10\n",
    "# Reset optimizer\n",
    "optimizer = torch.optim.Adam(graph.parameters(), lr=1e-3)\n",
    "\n",
    "STEPS = 100_000\n",
    "\n",
    "log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(data=\"train\", batch_size=BATCH_SIZE, train_examples=TRAIN_EXAMPLES, test_examples=TEST_EXAMPLES):\n",
    "    if data == \"train\":\n",
    "        data = train\n",
    "        classes = np.stack([np.random.choice(list(range(0, 8)), size=(n_classes), replace=False) for _ in range(batch_size)])\n",
    "    elif data == \"test\":\n",
    "        data = test\n",
    "        classes = np.stack([np.random.choice(list(range(8, 10)), size=(n_classes), replace=False) for _ in range(batch_size)])\n",
    "    \n",
    "    elif data == \"fashion\":\n",
    "        data = fashion_test\n",
    "        classes = np.stack([np.random.choice(list(range(10)), size=(n_classes), replace=False) for _ in range(batch_size)])\n",
    "    \n",
    "    y_train = np.random.randint(n_classes, size=(batch_size, train_examples))\n",
    "    x_train = []\n",
    "    for batch_classes, y in zip(classes, y_train):\n",
    "        x_train.append([])\n",
    "        for class_ in batch_classes[y]:\n",
    "            x_train[-1].append(random.choice(data[class_]))\n",
    "        x_train[-1] = np.stack(x_train[-1])\n",
    "    x_train = np.stack(x_train)\n",
    "\n",
    "    y_test = np.random.randint(n_classes, size=(batch_size, TEST_EXAMPLES))\n",
    "    x_test = []\n",
    "    for batch_classes, y in zip(classes, y_test):\n",
    "        x_test.append([])\n",
    "        for class_ in batch_classes[y]:\n",
    "            x_test[-1].append(random.choice(data[class_]))\n",
    "        x_test[-1] = np.stack(x_test[-1])\n",
    "    x_test = np.stack(x_test)\n",
    "    \n",
    "    return torch.Tensor(x_train).to(device), torch.Tensor(y_train).long().to(device), torch.Tensor(x_test).to(device), torch.Tensor(y_test).long().to(device), classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = tqdm(range(STEPS))\n",
    "for _ in bar:\n",
    "    x_train, y_train, x_test, y_test, classes = get_batch_data(data=\"train\")\n",
    "\n",
    "    graph.init_vals(nodes=True, edges=True, batch_size=BATCH_SIZE)\n",
    "    graph.detach_vals()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_input = nn.functional.one_hot(y_train, n_classes).float()\n",
    "\n",
    "    graph.learn(x_train, y_input, time=TIME, dt=DT)\n",
    "    pred = graph.predict(x_test, time=TIME, dt=DT)\n",
    "\n",
    "    accs = (pred.argmax(2) == y_test).float()\n",
    "\n",
    "    y_label = nn.functional.one_hot(y_test, n_classes).float()\n",
    "\n",
    "    task_loss = criterion(pred, y_label)\n",
    "    overflow = graph.overflow()\n",
    "\n",
    "    loss = task_loss + overflow\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(graph.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    entry = {'loss': task_loss.item(), 'acc': accs.mean().item(), \"overflow\": overflow.item(), \"decay\":graph.decay}\n",
    "    log.append(entry)\n",
    "    bar.set_postfix({\"loss\":np.mean([e[\"loss\"] for e in log[-10:]]), \"acc\":np.mean([e[\"acc\"] for e in log[-10:]]), \"overflow\":entry[\"overflow\"], \"decay\":entry[\"decay\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([e[\"loss\"] for e in log])\n",
    "# plt.ylim(0, .5)\n",
    "plt.show()\n",
    "\n",
    "plt.plot([e[\"acc\"] for e in log])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.save(\"models/generalize.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Train digits, Test digits, Fashion test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(SIZE**2, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(32, n_classes),\n",
    "            nn.Softmax(1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "def fit(model, x_train, y_train, epochs=3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for e in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x_train)\n",
    "        loss = criterion(y_hat, y_train)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b19b5b08e2459b980ac05ed7698b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m epochs \u001b[39min\u001b[39;00m [\u001b[39m10\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m1000\u001b[39m]:\n\u001b[0;32m     45\u001b[0m     model \u001b[39m=\u001b[39m Model()\n\u001b[1;32m---> 47\u001b[0m     fit(model, x_train_b, y_input_b, epochs\u001b[39m=\u001b[39mepochs)\n\u001b[0;32m     48\u001b[0m     pred \u001b[39m=\u001b[39m model(x_test_b)\n\u001b[0;32m     50\u001b[0m     model_acc \u001b[39m=\u001b[39m (pred\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m y_test_b)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\n",
      "Cell \u001b[1;32mIn[47], line 25\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, x_train, y_train, epochs)\u001b[0m\n\u001b[0;32m     22\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_hat, y_train)\n\u001b[0;32m     24\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 25\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Alec\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alec\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\Alec\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39meps\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mforeach\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39mgroup[\u001b[39m'\u001b[39m\u001b[39mfused\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgrad_scale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfound_inf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Alec\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\Alec\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    393\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "overall_accs = {}\n",
    "overall_losses = {}\n",
    "\n",
    "for data in [\"train\", \"test\", \"fashion\"]:\n",
    "    print(f\"Data: {data}\")\n",
    "\n",
    "    accs = {\"graph\":[], \"linreg\":[], \"ANN_10_epochs\":[], \"ANN_100_epochs\":[], \"ANN_1000_epochs\":[]}\n",
    "    losses = {\"graph\":[], \"linreg\":[], \"ANN_10_epochs\":[], \"ANN_100_epochs\":[], \"ANN_1000_epochs\":[]}\n",
    "\n",
    "    # Average across 10 tries with batch_size=32 so N=320\n",
    "    for _ in tqdm(range(10)):\n",
    "        x_train, y_train, x_test, y_test, classes = get_batch_data(data=data)\n",
    "        y_input = nn.functional.one_hot(y_train, n_classes).float()\n",
    "        y_label = nn.functional.one_hot(y_test, n_classes).float()\n",
    "        \n",
    "        # Try graph\n",
    "        with torch.no_grad():\n",
    "            graph.init_vals(nodes=True, edges=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "            graph.learn(x_train, y_input, time=TIME, dt=DT)\n",
    "            pred = graph.predict(x_test, time=TIME, dt=DT)\n",
    "\n",
    "            graph_acc = (pred.argmax(2) == y_test).float().mean()\n",
    "            graph_loss = criterion(pred, y_label)\n",
    "\n",
    "            accs[\"graph\"].append(graph_acc.item())\n",
    "            losses[\"graph\"].append(graph_loss.item())\n",
    "\n",
    "        # Try linear regression and an ANN on each batch\n",
    "        for x_train_b, y_train_b, x_test_b, y_test_b, y_input_b, y_label_b in zip(x_train, y_train, x_test, y_test, y_input, y_label):\n",
    "            model = LinearRegression()\n",
    "\n",
    "            model.fit(x_train_b.cpu(), y_input_b.cpu())\n",
    "            pred = torch.Tensor(model.predict(x_test_b.cpu())).to(device)\n",
    "\n",
    "            linreg_acc = (pred.argmax(1) == y_test_b).float().mean()\n",
    "            linreg_loss = criterion(pred, y_label_b)\n",
    "\n",
    "            accs[\"linreg\"].append(linreg_acc.item())\n",
    "            losses[\"linreg\"].append(linreg_loss.item())\n",
    "\n",
    "\n",
    "            # Train model for 10, 100, 1000 epochs and record acc / loss\n",
    "            for epochs in [10, 100, 1000]:\n",
    "                model = Model().to(device)\n",
    "\n",
    "                fit(model, x_train_b, y_input_b, epochs=epochs)\n",
    "                pred = model(x_test_b)\n",
    "\n",
    "                model_acc = (pred.argmax(1) == y_test_b).float().mean()\n",
    "                model_loss = criterion(pred, y_label_b)\n",
    "\n",
    "                accs[f\"ANN_{epochs}_epochs\"].append(model_acc.item())\n",
    "                losses[f\"ANN_{epochs}_epochs\"].append(model_loss.item())\n",
    "\n",
    "    overall_accs[data] = {k:np.mean(v) for k, v in accs.items()}\n",
    "    overall_losses[data] = {k:np.mean(v) for k, v in losses.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=.05\n",
    "\n",
    "plt.title(\"Model vs Accuracy from 10 train examples\")\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "X = overall_accs.keys()\n",
    "X_axis = np.arange(len(X))\n",
    "\n",
    "plt.xticks(X_axis, X)\n",
    "\n",
    "model_names = overall_accs[\"train\"].keys()\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    accs = []\n",
    "    for data_set in X:\n",
    "        accs.append(overall_accs[data_set][model_name])\n",
    "    plt.bar(X_axis + i*(width) - len(model_names)*(width/2), accs, width, label=model_name)\n",
    "    \n",
    "    \n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"results.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
